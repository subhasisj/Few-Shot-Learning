{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('nlp-generic': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b481e22569f8c970d02674a5d9e45918cde1d33fcb13233e32a75967a14c2314"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import itertools\n",
    "import texthero as hero\n",
    "# from zeugma import EmbeddingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./final_fewshot_train.csv')\n",
    "df_test = pd.read_csv('./final_fewshot_test.csv')\n",
    "df_train=df_train[['text','class']]\n",
    "df_test=df_test[['text','class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             text  class\n",
       "0           [ALLUXIO-2743] Fix failing unit tests      1\n",
       "1             #2 Refactored structure of Argument      3\n",
       "2         Remove some features from JwtTokenStore      4\n",
       "3  Remove duplicated 1.613 section from changelog      2\n",
       "4                  * webapp structure refactoring      3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[ALLUXIO-2743] Fix failing unit tests</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2 Refactored structure of Argument</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Remove some features from JwtTokenStore</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Remove duplicated 1.613 section from changelog</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>* webapp structure refactoring</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       text\n",
       "class      \n",
       "1        20\n",
       "2        20\n",
       "3        20\n",
       "4        20\n",
       "5        20"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>class</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train.groupby('class').count()"
   ]
  },
  {
   "source": [
    "We have 20 examples each for 5 classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3277 entries, 0 to 3276\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    3277 non-null   object\n 1   class   3277 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 51.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 3, 4, 2, 5], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "labels = df_train['class'].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "# def text_cleaner(s):\n",
    "#     s = hero.remove_digits(s)\n",
    "#     s = hero.remove_brackets(s)\n",
    "#     s = hero.remove_punctuation(s)\n",
    "#     s = hero.remove_whitespace(s)\n",
    "#     s = hero.remove_stopwords(s)\n",
    "\n",
    "#     return s\n",
    "\n",
    "df_train['cleaned_text'] = hero.clean(df_train['text'])\n",
    "df_test['cleaned_text'] = hero.clean(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             text  class  \\\n",
       "0           [ALLUXIO-2743] Fix failing unit tests      1   \n",
       "1             #2 Refactored structure of Argument      3   \n",
       "2         Remove some features from JwtTokenStore      4   \n",
       "3  Remove duplicated 1.613 section from changelog      2   \n",
       "4                  * webapp structure refactoring      3   \n",
       "\n",
       "                          cleaned_text  \n",
       "0       alluxio fix failing unit tests  \n",
       "1        refactored structure argument  \n",
       "2        remove features jwttokenstore  \n",
       "3  remove duplicated section changelog  \n",
       "4         webapp structure refactoring  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[ALLUXIO-2743] Fix failing unit tests</td>\n      <td>1</td>\n      <td>alluxio fix failing unit tests</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2 Refactored structure of Argument</td>\n      <td>3</td>\n      <td>refactored structure argument</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Remove some features from JwtTokenStore</td>\n      <td>4</td>\n      <td>remove features jwttokenstore</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Remove duplicated 1.613 section from changelog</td>\n      <td>2</td>\n      <td>remove duplicated section changelog</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>* webapp structure refactoring</td>\n      <td>3</td>\n      <td>webapp structure refactoring</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_left = []\n",
    "text_right = []\n",
    "target = []\n",
    "\n",
    "\n",
    "for label in labels:\n",
    "    \n",
    "    similar_texts = df_train[df_train['class']==label]['cleaned_text']\n",
    "    group_similar_texts = list(itertools.combinations(similar_texts,2))\n",
    "    \n",
    "    text_left.extend([group[0] for group in group_similar_texts])\n",
    "    text_right.extend([group[1] for group in group_similar_texts])\n",
    "    target.extend([1.]*len(group_similar_texts))\n",
    "\n",
    "    dissimilar_texts = df_train[df_train['class']!=label]['cleaned_text']\n",
    "    for i in range(len(group_similar_texts)):\n",
    "        text_left.append(np.random.choice(similar_texts))\n",
    "        text_right.append(np.random.choice(dissimilar_texts))\n",
    "        target.append(0.)\n",
    "        \n",
    "dataset = pd.DataFrame({'text_left':text_left,\n",
    "                    'text_right':text_right,\n",
    "                    'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              text_left  \\\n",
       "203   cloudstack ui network guest network ip address...   \n",
       "738   reverted earlier design change allow use regul...   \n",
       "1791  improved mmap management buffer pool full perf...   \n",
       "960                       updated new grouping features   \n",
       "1521  improved performance clearing map instead recr...   \n",
       "412                        webapp structure refactoring   \n",
       "1195  smallfix fix duplicated properties alias prope...   \n",
       "70                      related ui change api bug fixed   \n",
       "1284                small refactor avoid duplicate code   \n",
       "1309                    remove duplicate obsolete tests   \n",
       "\n",
       "                                             text_right  target  \n",
       "203   add notes coptic bug fix scalar performance im...     0.0  \n",
       "738   improved mmap management buffer pool full perf...     0.0  \n",
       "1791  introduced thunk structure intermediate repres...     0.0  \n",
       "960                    docearevent structure refactored     0.0  \n",
       "1521  add notes coptic bug fix scalar performance im...     1.0  \n",
       "412   introduced thunk structure intermediate repres...     1.0  \n",
       "1195  remove duplicate scripts move remaining items ...     1.0  \n",
       "70    npe payload causing ssvm agent fix also make s...     1.0  \n",
       "1284  need duplicate close method already inherited ...     1.0  \n",
       "1309  commandlinerunner handle uris refactored dupli...     1.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_left</th>\n      <th>text_right</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>203</th>\n      <td>cloudstack ui network guest network ip address...</td>\n      <td>add notes coptic bug fix scalar performance im...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>738</th>\n      <td>reverted earlier design change allow use regul...</td>\n      <td>improved mmap management buffer pool full perf...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1791</th>\n      <td>improved mmap management buffer pool full perf...</td>\n      <td>introduced thunk structure intermediate repres...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>960</th>\n      <td>updated new grouping features</td>\n      <td>docearevent structure refactored</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1521</th>\n      <td>improved performance clearing map instead recr...</td>\n      <td>add notes coptic bug fix scalar performance im...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>webapp structure refactoring</td>\n      <td>introduced thunk structure intermediate repres...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>smallfix fix duplicated properties alias prope...</td>\n      <td>remove duplicate scripts move remaining items ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>related ui change api bug fixed</td>\n      <td>npe payload causing ssvm agent fix also make s...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1284</th>\n      <td>small refactor avoid duplicate code</td>\n      <td>need duplicate close method already inherited ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1309</th>\n      <td>remove duplicate obsolete tests</td>\n      <td>commandlinerunner handle uris refactored dupli...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1900 entries, 0 to 1899\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   text_left   1900 non-null   object \n 1   text_right  1900 non-null   object \n 2   target      1900 non-null   float64\ndtypes: float64(1), object(2)\nmemory usage: 44.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "source": [
    "From a training set of 100 samples were able to create 1900 samples for training the siamese network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Subtract, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 100\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 584 unique tokens.\n",
      "(1900, 100)\n",
      "(1900, 100)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train.cleaned_text)\n",
    "sequences_left = tokenizer.texts_to_sequences(dataset.text_left)\n",
    "sequences_right = tokenizer.texts_to_sequences(dataset.text_right)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "x_left = pad_sequences(sequences_left, maxlen=MAX_SEQ_LENGTH)\n",
    "x_right = pad_sequences(sequences_right, maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "print(x_left.shape)\n",
    "print(x_right.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(arms_difference):\n",
    "    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left/right inputs\n",
    "    similarity from the inputs differences. This function is used to turn the unbounded\n",
    "    L1 distance to a similarity measure between 0 and 1\"\"\"\n",
    "\n",
    "    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('./glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(VOCAB_SIZE, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > VOCAB_SIZE:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_left (InputLayer)         [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ninput_right (InputLayer)        [(None, 100)]        0                                            \n__________________________________________________________________________________________________\nsequential_network (Sequential) (None, 64)           92548       input_left[0][0]                 \n                                                                 input_right[0][0]                \n__________________________________________________________________________________________________\npair_representations_difference (None, 64)           0           sequential_network[0][0]         \n                                                                 sequential_network[1][0]         \n__________________________________________________________________________________________________\nmasltsm_distance (Lambda)       (None, 1)            0           pair_representations_difference[0\n==================================================================================================\nTotal params: 92,548\nTrainable params: 34,048\nNon-trainable params: 58,500\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def siamese_lstm_model(max_length):\n",
    "\n",
    "    input_shape = (max_length,)\n",
    "    input_left = Input(input_shape,name = 'input_left')\n",
    "    input_right = Input(input_shape,name = 'input_right')\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix),\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)\n",
    "\n",
    "    seq = Sequential(name='sequential_network')\n",
    "    seq.add(embedding_layer)\n",
    "    seq.add(Bidirectional(LSTM(32, dropout=0.3, recurrent_dropout=0.)))\n",
    "\n",
    "    output_left = seq(input_left)\n",
    "    output_right = seq(input_right)\n",
    "\n",
    "    # Here we subtract the neuron values of the last layer from the left arm \n",
    "    # with the corresponding values from the right arm\n",
    "\n",
    "    subtracted = Subtract(name='pair_representations_difference')([output_left, output_right])\n",
    "    malstm_distance = Lambda(exponent_neg_manhattan_distance, \n",
    "                             name='masltsm_distance')(subtracted)\n",
    "\n",
    "    siamese_net = Model(inputs=[input_left, input_right], outputs=malstm_distance)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "\n",
    "siamese_lstm = siamese_lstm_model(MAX_SEQ_LENGTH)\n",
    "siamese_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/12\n",
      "42/42 [==============================] - 2s 48ms/step - loss: 1.9881 - accuracy: 0.4286 - val_loss: 0.7603 - val_accuracy: 0.6702\n",
      "Epoch 2/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 1.1891 - accuracy: 0.4293 - val_loss: 0.7000 - val_accuracy: 0.6737\n",
      "Epoch 3/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.9256 - accuracy: 0.4286 - val_loss: 0.7158 - val_accuracy: 0.6175\n",
      "Epoch 4/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.8174 - accuracy: 0.4474 - val_loss: 0.7438 - val_accuracy: 0.5246\n",
      "Epoch 5/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.7498 - accuracy: 0.4827 - val_loss: 0.7605 - val_accuracy: 0.4439\n",
      "Epoch 6/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.7049 - accuracy: 0.5353 - val_loss: 0.7756 - val_accuracy: 0.4298\n",
      "Epoch 7/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.6660 - accuracy: 0.5880 - val_loss: 0.7759 - val_accuracy: 0.4351\n",
      "Epoch 8/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.6373 - accuracy: 0.6429 - val_loss: 0.7757 - val_accuracy: 0.4544\n",
      "Epoch 9/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.6094 - accuracy: 0.6857 - val_loss: 0.7737 - val_accuracy: 0.4649\n",
      "Epoch 10/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.5836 - accuracy: 0.7301 - val_loss: 0.7717 - val_accuracy: 0.4649\n",
      "Epoch 11/12\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.5560 - accuracy: 0.7662 - val_loss: 0.7657 - val_accuracy: 0.4807\n",
      "Epoch 12/12\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.5332 - accuracy: 0.8090 - val_loss: 0.7517 - val_accuracy: 0.4930\n"
     ]
    }
   ],
   "source": [
    "siamese_lstm.fit([x_left,x_right], dataset.target, validation_split=0.3, epochs=12);"
   ]
  },
  {
   "source": [
    "## Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_sequences = tokenizer.texts_to_sequences(df_train.cleaned_text)\n",
    "x_reference_sequences = pad_sequences(reference_sequences, maxlen=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def flatten_text_sequence(text):\n",
    "    flatten = itertools.chain.from_iterable\n",
    "    text = list(flatten(text))\n",
    "    return text\n",
    "\n",
    "def get_prediction(text):\n",
    "    \"\"\" Get the predicted category, and the most similar text\n",
    "    in the train set. Note that this way of computing a prediction is highly \n",
    "    not optimal, but it'll be sufficient for us now. \"\"\"\n",
    "    x = tokenizer.texts_to_sequences(text.split())\n",
    "    x = flatten_text_sequence(x)\n",
    "    x = pad_sequences([x], maxlen=MAX_SEQ_LENGTH)\n",
    "    # x = np.array(x)\n",
    "    # print([x[0]]*len(x_reference_sequences))\n",
    "    # print(x_reference_sequences.shape)\n",
    "    # Compute similarities of the text with all text's in the train set\n",
    "    result = np.repeat(x, len(x_reference_sequences), axis=0)\n",
    "    # similarities = siamese_lstm.predict([[x[0]]*len(x_reference_sequences), x_reference_sequences])\n",
    "    similarities = siamese_lstm.predict([result, x_reference_sequences])\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    \n",
    "    # The predicted category is the one of the most similar example from the train set\n",
    "    # print(most_similar_index)\n",
    "    prediction = df_train['class'].iloc[most_similar_index]\n",
    "    most_similar_example = df_train['cleaned_text'].iloc[most_similar_index]\n",
    "\n",
    "    return prediction, most_similar_example"
   ]
  },
  {
   "source": [
    "https://github.com/amansrivastava17/lstm-siamese-text-similarity\n",
    "\n",
    "https://github.com/nkthiebaut/nkthiebaut.github.io/blob/source/content/fewshot_learning_nlp.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "x  = df_train['cleaned_text'].iloc[34]\n",
    "# print(x)\n",
    "x = tokenizer.texts_to_sequences(x.split())\n",
    "x = flatten_text_sequence(x)\n",
    "x = pad_sequences([x], maxlen=MAX_SEQ_LENGTH)  \n",
    "# x\n",
    "result = np.repeat(x, len(x_reference_sequences), axis=0)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sampled Text: revert cloudstack automation fix test failure test 02 revert vm snapshots smoke test vm snapshots py\nTrue Class: 1\nPredicted Class : 1\nMost similar example in train set: automation fix test failure test 02 revert vm snapshots smoke test vm snapshots py\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 22\n",
    "\n",
    "pred, most_sim = get_prediction(df_test.cleaned_text[sample_idx])\n",
    "\n",
    "print(f'Sampled Text: {df_test[\"cleaned_text\"].iloc[sample_idx]}')\n",
    "print(f'True Class: {df_test[\"class\"].iloc[sample_idx]}')\n",
    "print(f'Predicted Class : {pred}')\n",
    "print(f'Most similar example in train set: {most_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# df_eval = df_test[:50]\n",
    "\n",
    "# y_pred = [get_prediction(text)[0] for text in df_eval['cleaned_text']]\n",
    "# accuracy = accuracy_score(y_pred, df_eval['class'])\n",
    "\n",
    "# print(f'Test accuracy (siamese model): {100*accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}